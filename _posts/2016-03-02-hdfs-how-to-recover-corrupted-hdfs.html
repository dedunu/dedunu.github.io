---
layout: post
title: HDFS - How to recover corrupted HDFS metadata in Hadoop 1.2.X?
date: '2016-03-02T21:22:00.001+02:00'
author: Dedunu Dhananjaya
tags:
- metadata
- recover
- hadoop
- linux
- hdfs
modified_time: '2019-07-09T18:19:08.966+03:00'
blogger_id: tag:blogger.com,1999:blog-8327060682734922468.post-6416364578014973621
permalink: /2016/03/hdfs-how-to-recover-corrupted-hdfs.html
---

<div dir="ltr" style="text-align: left;" trbidi="on"><div style="text-align: justify;">You might have Hadoop in your production. And sometimes Tera-bytes of data is residing in Hadoop. HDFS metadata can get corrupted. Namenode won't start in such cases. When you check Namenode logs you might see exceptions.</div><div style="text-align: justify;"><br /></div><div style="text-align: justify;"><pre>ERROR org.apache.hadoop.dfs.NameNode: java.io.EOFException<br /><span class="anchor" id="line-2"></span>    at java.io.DataInputStream.readFully(DataInputStream.java:178)<br /><span class="anchor" id="line-3"></span>    at org.apache.hadoop.io.UTF8.readFields(UTF8.java:106)<br /><span class="anchor" id="line-4"></span>    at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:90)<br /><span class="anchor" id="line-5"></span>    at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:433)<br /><span class="anchor" id="line-6"></span>    at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:759)<br /><span class="anchor" id="line-7"></span>    at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:639)<br /><span class="anchor" id="line-8"></span>    at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:222)<br /><span class="anchor" id="line-9"></span>    at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:79)<br /><span class="anchor" id="line-10"></span>    at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:254)<br /><span class="anchor" id="line-11"></span>    at org.apache.hadoop.dfs.FSNamesystem.&lt;init&gt;(FSNamesystem.java:235)<br /><span class="anchor" id="line-12"></span>    at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)<br /><span class="anchor" id="line-13"></span>    at org.apache.hadoop.dfs.NameNode.&lt;init&gt;(NameNode.java:176)<br /><span class="anchor" id="line-14"></span>    at org.apache.hadoop.dfs.NameNode.&lt;init&gt;(NameNode.java:162)<br /><span class="anchor" id="line-15"></span>    at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:846)<br /><span class="anchor" id="line-16"></span>    at org.apache.hadoop.dfs.NameNode.main(NameNode.java:855)</pre><pre></pre></div><div style="text-align: justify;">If you have a development environment, you can always format the HDFS and continue. This blog posts even suggest that - <a href="https://autofei.wordpress.com/2011/03/27/hadoop-namenode-failed-and-reset/">https://autofei.wordpress.com/2011/03/27/hadoop-namenode-failed-and-reset/</a></div><div style="text-align: justify;"><br /></div><div style="text-align: justify;"><span style="color: red;">BUT IF YOU FORMAT HDFS YOU lose ALL THE FILES IN HDFS!!!&nbsp;</span></div><div style="text-align: justify;"><br /></div><div style="text-align: justify;">So Hadoop Administrators can't format HDFS simply. But you can recover your HDFS to last checkpoint. You might lose some data files. But more than 90% of the data might be safe. Let's see how to recover corrupted HDFS metadata.</div><div style="text-align: justify;"><br /></div><div style="text-align: justify;">Hadoop is creating checkpoints periodically in Namenode folder. You might see three folders in Namenode directory. They are</div><ol style="text-align: justify;"><li>current</li><li>image</li><li>previous.checkpoint</li></ol><div style="text-align: justify;"><br />the current folder must be corrupted most probably.</div><ul style="text-align: justify;"><li>Stop all the Hadoop services from all the nodes. </li><li>Backup both "current" and "previous.checkpoint" directories.&nbsp;</li><li>Delete "current" directory.&nbsp;</li><li>Rename "previous.checkpoint" to "current"</li><li>Restart Hadoop services.&nbsp;</li></ul><br /><ul style="text-align: justify;"></ul><div style="text-align: justify;">Steps I followed I have mentioned above. Below commands were run to recover the HDFS. Commands might slightly change depending on your installation.</div><div style="text-align: justify;"><br /></div><div style="text-align: justify;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">/usr/local/hadoop/stop-all.sh</span></div><div style="text-align: justify;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">cd &lt;namenode.dir&gt;</span></div><div style="text-align: justify;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">cp -r current current.old</span></div><div style="text-align: justify;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">cp -r previous.checkpoint previous.checkpoint.old</span></div><div style="text-align: justify;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">mv previous.checkpoint current</span></div><div style="text-align: justify;"><span style="font-family: &quot;courier new&quot; , &quot;courier&quot; , monospace;">/usr/local/hadoop/start-all.sh</span></div><div style="text-align: justify;"><br /></div><div style="text-align: justify;">That's all!!!! Everything was okay after that!</div><br /></div>